\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{hyperref}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}

\title{RL-AG Methodology}
\author{}
\date{}

\begin{document}

\maketitle

\section{Methodology}

\subsection{Formal Model}
We cast reinforcement-learning attack-path generation as a Markov decision process (MDP)
\(
M = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle
\).
Each state \(s_t \in \mathcal{S}\) is a heterogeneous graph \(G_t = \langle G_{\text{net}}, IAG_t \rangle\) composed of
(i) a static industrial topology \(G_{\text{net}} = (\mathcal{V}_{\text{net}}, \mathcal{E}_{\text{net}})\) whose nodes encode device attributes and mapped MITRE techniques, and
(ii) a dynamic provenance graph \(IAG_t = (\mathcal{V}_{\text{prov}}^t, \mathcal{E}_{\text{prov}}^t)\) over the TAGAPT entity alphabet \(\mathcal{C} = \{\text{MP}, \text{TP}, \text{MF}, \text{SF}, \text{TF}, \text{SO}\}\).
We denote the observation encoder as \(\phi: \mathcal{S} \rightarrow \mathbb{R}^{d_{\text{in}} \times |\mathcal{V}_{\text{prov}}|}\), mapping each entity to a feature vector, and represent edge relations via an index function \(\psi: \mathcal{E}_{\text{prov}} \rightarrow \{1,\dots,R\}\) learned from TAGAPT triples.

The macro-action space factorizes into stages, ATT\&CK techniques, and targeted devices:
\begin{equation}
    \mathcal{A} = \bigcup_{k \in \mathcal{T}} \left\{ (k, \tau, v) \mid \tau \in \mathcal{U}_k,\; v \in \mathcal{D}(\tau) \right\} \cup \mathcal{A}_{\text{skip}},
\end{equation}
where \(\mathcal{T}\) is the ordered set of stages, \(\mathcal{U}_k\) lists tactics admissible under TAGAPT dependencies at stage \(k\), and \(\mathcal{D}(\tau)\) enumerates devices whose platform supports technique \(\tau\).
Optional-only stages admit auxiliary skip actions \(\mathcal{A}_{\text{skip}}\) that ensure curriculum progression without fabricating provenance events.

Transition dynamics follow from executing a macro-action \(a_t = (k_t, \tau_t, v_t)\):
the environment deterministically applies the TAGAPT provenance triples associated with \(\tau_t\) to \(IAG_t\), subject to domain constraints,
and then updates stage-availability sets, device states, and per-stage patience counters.
Stochasticity arises from instance sampling and observation noise induced by action masking.
We denote the transition kernel as
\(
    \mathcal{P}(s_{t+1} \mid s_t, a_t) = \Prob\big(G_{t+1} \mid G_t, a_t\big).
\)

Rewards decompose into structural, temporal, and utility components:
\begin{align}
    R_{\text{struct}}(s_t, a_t) &= \alpha \sum_{e \in \Delta \mathcal{E}_{\text{prov}}} \mathbb{1}[e \text{ valid}] - \beta \sum_{e \in \Delta \mathcal{E}_{\text{prov}}} \mathbb{1}[e \text{ violates constraints}], \\
    R_{\text{temp}}(s_t, a_t) &= \eta \sum_{k \in \Delta \mathcal{K}_{\text{unlock}}} w_k + \zeta \sum_{k \in \Delta \mathcal{K}_{\text{complete}}} w_k - \lambda \cdot \max(0, \tau_t - \tau_{\text{patience}}), \\
    R_{\text{util}}(s_t, a_t) &= -c_{\text{act}} + b_{\text{impact}} \cdot \mathbb{1}[k_t = k_{\text{terminal}}],
\end{align}
where \(w_k\) is a stage-dependent multiplier, \(\tau_t\) counts steps since last progress, and \(\alpha,\beta,\eta,\zeta,\lambda,c_{\text{act}},b_{\text{impact}}\) are design constants.
The total reward satisfies \(r_t = R_{\text{struct}} + R_{\text{temp}} + R_{\text{util}}\).
Optional reward normalization applies a running estimate of mean \(\mu_t\) and variance \(\sigma_t^2\) such that \(\tilde{r}_t = (r_t - \mu_t)/(\sigma_t + \epsilon)\).

\subsection{Policy Parameterization}
We parameterize the policy with a relational encoder \(f_{\theta}: \mathcal{S} \rightarrow \mathbb{R}^{d}\) that aggregates node features through \(L\) relational graph convolution layers.
Let \(H^{(0)} \in \mathbb{R}^{|\mathcal{V}_{\text{prov}}| \times d_{\text{in}}}\) be the encoded node matrix.
For \(\ell = 1,\dots,L\),
\begin{equation}
    H^{(\ell)}_i = \sigma \left( \sum_{r=1}^{R} \sum_{j \in \mathcal{N}_r(i)} \frac{1}{|\mathcal{N}_r(i)|} W^{(\ell)}_r H^{(\ell-1)}_j + B^{(\ell)} \right),
\end{equation}
where \(\mathcal{N}_r(i)\) denotes neighbors of node \(i\) under relation \(r\), \(W^{(\ell)}_r \in \mathbb{R}^{d \times d}\) are relation-specific weights, \(B^{(\ell)}\) is a shared bias, and \(\sigma(\cdot)\) is a pointwise nonlinearity.
The graph embedding is \(z = \frac{1}{|\mathcal{V}_{\text{prov}}|} \sum_i H^{(L)}_i\).

Actor and critic heads are linear maps:
\(
    \pi_{\theta}(a \mid s) = \text{softmax}\left( W_{\pi} z + b_{\pi} \right), \quad
    V_{\theta}(s) = W_{V} z + b_{V}.
\)
Action masking is encoded by an indicator vector \(m(s) \in \{0, -\infty\}^{|\mathcal{A}|}\) that sets logits of invalid actions to \(-\infty\), yielding the masked distribution
\(
    \pi_{\theta}^{\text{mask}}(a \mid s) = \frac{\exp(\ell_a + m_a(s))}{\sum_{a'} \exp(\ell_{a'} + m_{a'}(s))}
\)
with logits \(\ell = W_{\pi} z + b_{\pi}\).
Sampling uses the categorical distribution for exploration, while evaluation employs \(a_t = \arg\max_a \pi_{\theta}^{\text{mask}}(a \mid s_t)\).

\subsection{Optimization Procedure}
We collect rollouts of length \(T\) and compute Generalized Advantage Estimation (GAE):
\begin{align}
    \delta_t &= \tilde{r}_t + \gamma V_{\theta}(s_{t+1}) - V_{\theta}(s_t), \\
    \hat{A}_t &= \delta_t + \gamma \lambda \hat{A}_{t+1},
\end{align}
with terminal condition \(\hat{A}_{T} = 0\).
Let \(\hat{R}_t = \hat{A}_t + V_{\theta}(s_t)\) be the bootstrapped return.

The PPO-Clip objective maximized over mini-batches \(\mathcal{B}\) is
\begin{equation}
    \mathcal{L}^{\text{PPO}}(\theta) = \frac{1}{|\mathcal{B}|} \sum_{t \in \mathcal{B}} \min \left(
        \rho_t(\theta) \hat{A}_t,\;
        \text{clip}\big( \rho_t(\theta), 1-\epsilon, 1+\epsilon \big) \hat{A}_t
    \right)
    - c_v (V_{\theta}(s_t) - \hat{R}_t)^2 + c_e \mathcal{H}\big(\pi_{\theta}^{\text{mask}}(\cdot \mid s_t)\big),
\end{equation}
where \(\rho_t(\theta) = \frac{\pi_{\theta}^{\text{mask}}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}^{\text{mask}}(a_t \mid s_t)}\),
\(\epsilon\) is the clip coefficient, \(c_v\) and \(c_e\) weight the value and entropy terms, and \(\mathcal{H}\) denotes categorical entropy.
Gradient updates apply Adam with optional gradient-norm clipping.

Curriculum scheduling defines a stage subset \(\mathcal{T}_p\) for phase \(p\) and tracks success statistics via a sliding window of size \(K\):
\begin{equation}
    \sigma_p = \frac{1}{K} \sum_{t=1}^{K} \left( \mathbb{1}[k_t \in \mathcal{T}_p \text{ completed}] + \mathbb{1}[\text{episode terminates in } \mathcal{T}_p] \right).
\end{equation}
The curriculum advances to phase \(p+1\) once \(\sigma_p \geq \tau_{\text{succ}}\) and a minimum number of rollouts \(N_p\) has elapsed; the environment is reset with the new stage order thereafter.

\subsection{Evaluation Metrics}
For a set of evaluation episodes \(\{ \xi_i \}_{i=1}^M\), we compute:
\begin{align}
    \bar{J} &= \frac{1}{M} \sum_{i=1}^{M} \sum_{t \in \xi_i} r_t, && \text{(average cumulative reward)} \\
    \bar{L} &= \frac{1}{M} \sum_{i=1}^{M} |\xi_i|, && \text{(average episode length)} \\
    \rho_{\text{succ}} &= \frac{1}{M} \sum_{i=1}^{M} \mathbb{1}[k_{\text{terminal}} \text{ reached in } \xi_i], && \text{(terminal-stage success rate)} \\
    D_{\text{tech}} &= \left| \bigcup_{i=1}^{M} \{\tau_t : t \in \xi_i\} \right|, && \text{(technique diversity).}
\end{align}
When exporting trajectories, we optionally retain the top-\(K\) episodes under reward ordering.
The PLC-impact filter evaluates predicate \(g(\xi_i) = 1\) if an impact-stage action targets any device with type ``PLC''; only episodes with \(g(\xi_i)=1\) are persisted when \(K_{\text{PLC}}\) is requested.

\section*{Acknowledgements}
This document summarizes the mathematical formulation underlying the RL-AG system for Overleaf-ready dissemination.

\end{document}

